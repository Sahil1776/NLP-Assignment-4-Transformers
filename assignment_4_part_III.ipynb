{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/M3_Assignment/blob/main/scripts/m3_assignment_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVQ8K7gthTsX",
        "outputId": "93733deb-7aec-4af4-f836-9c1f3ea79cf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "names.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_D3r13Agwak",
        "outputId": "ebf6e22d-777c-4573-f970-ff687e0ade0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['female.txt', 'male.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')"
      ],
      "metadata": {
        "id": "H0J3Ji_wgwdr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Encoder-Decoder**"
      ],
      "metadata": {
        "id": "HobqRF3txWWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Assuming male_names and female_names are already prepared lists\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "names = [(name, 'male') for name in male_names] + [(name, 'female') for name in female_names]\n",
        "names_train, names_val = train_test_split(names, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, names):\n",
        "        self.names = names\n",
        "        self.max_seq_length = 20  # Define your desired maximum sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name, label = self.names[idx]\n",
        "        encoding = tokenizer(name, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_seq_length)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        label = 1 if label == 'female' else 0  # Encoding labels as 0 for male, 1 for female\n",
        "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_dataset = NameDataset(names_train)\n",
        "val_dataset = NameDataset(names_val)\n",
        "\n",
        "# Step 3: Model Architecture\n",
        "class NameClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model_name='bert-base-uncased', num_classes=2):\n",
        "        super(NameClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Use the pooled output from BERT\n",
        "        output = self.dropout(pooled_output)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Step 4: Training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NameClassifier()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Step 5: Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in val_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Step 6: Inference\n",
        "def predict_gender(name):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(name, return_tensors='pt', padding='max_length', truncation=True, max_length=20)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return 'female' if predicted.item() == 1 else 'male'\n",
        "\n",
        "# Example usage\n",
        "name = \"John\"\n",
        "predicted_gender = predict_gender(name)\n",
        "print(f'The predicted gender for the name \"{name}\" is: {predicted_gender}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTqJiTVm1QvK",
        "outputId": "95ae3e77-71e7-4e32-a827-544b0aeddf19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.4022\n",
            "Epoch 2/5, Loss: 0.2737\n",
            "Epoch 3/5, Loss: 0.2334\n",
            "Epoch 4/5, Loss: 0.1917\n",
            "Epoch 5/5, Loss: 0.1673\n",
            "Validation Accuracy: 0.8628\n",
            "The predicted gender for the name \"John\" is: male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**"
      ],
      "metadata": {
        "id": "GI-gMgrJxTp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define input sequence length\n",
        "max_seq_length = 20\n",
        "\n",
        "# Tokenize the names\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(male_names + female_names)\n",
        "\n",
        "# Convert names to sequences\n",
        "male_sequences = tokenizer.texts_to_sequences(male_names)\n",
        "female_sequences = tokenizer.texts_to_sequences(female_names)\n",
        "\n",
        "# Pad sequences to ensure fixed length\n",
        "male_padded_sequences = pad_sequences(male_sequences, maxlen=max_seq_length, padding='post')\n",
        "female_padded_sequences = pad_sequences(female_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Create labels\n",
        "male_labels = np.ones(len(male_padded_sequences))\n",
        "female_labels = np.zeros(len(female_padded_sequences))\n",
        "\n",
        "# Combine male and female data\n",
        "X_data = np.concatenate((male_padded_sequences, female_padded_sequences), axis=0)\n",
        "y_data = np.concatenate((male_labels, female_labels), axis=0)\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.arange(len(X_data))\n",
        "np.random.shuffle(indices)\n",
        "X_data = X_data[indices]\n",
        "y_data = y_data[indices]\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_size = int(0.7 * len(X_data))\n",
        "\n",
        "X_train = X_data[:train_size]\n",
        "y_train = y_data[:train_size]\n",
        "\n",
        "X_val = X_data[train_size:]\n",
        "y_val = y_data[train_size:]\n",
        "\n",
        "# Reshape input for LSTM model\n",
        "X_train = X_train.reshape(-1, max_seq_length, 1)\n",
        "X_val = X_val.reshape(-1, max_seq_length, 1)\n"
      ],
      "metadata": {
        "id": "ejE7lNe7p9uG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(max_seq_length, 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBMcxnGOp9xO",
        "outputId": "93ff96b9-13f1-4b06-911c-8d6b49bbf45c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "174/174 [==============================] - 5s 18ms/step - loss: 0.6443 - accuracy: 0.6363 - val_loss: 0.6185 - val_accuracy: 0.6628\n",
            "Epoch 2/20\n",
            "174/174 [==============================] - 3s 18ms/step - loss: 0.6192 - accuracy: 0.6552 - val_loss: 0.6127 - val_accuracy: 0.6648\n",
            "Epoch 3/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.6079 - accuracy: 0.6655 - val_loss: 0.6110 - val_accuracy: 0.6527\n",
            "Epoch 4/20\n",
            "174/174 [==============================] - 3s 16ms/step - loss: 0.5935 - accuracy: 0.6752 - val_loss: 0.5837 - val_accuracy: 0.6946\n",
            "Epoch 5/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.5707 - accuracy: 0.7007 - val_loss: 0.5654 - val_accuracy: 0.7278\n",
            "Epoch 6/20\n",
            "174/174 [==============================] - 3s 18ms/step - loss: 0.5395 - accuracy: 0.7338 - val_loss: 0.5304 - val_accuracy: 0.7471\n",
            "Epoch 7/20\n",
            "174/174 [==============================] - 3s 19ms/step - loss: 0.5154 - accuracy: 0.7437 - val_loss: 0.5283 - val_accuracy: 0.7328\n",
            "Epoch 8/20\n",
            "174/174 [==============================] - 3s 16ms/step - loss: 0.5109 - accuracy: 0.7514 - val_loss: 0.5211 - val_accuracy: 0.7471\n",
            "Epoch 9/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.5025 - accuracy: 0.7523 - val_loss: 0.5114 - val_accuracy: 0.7521\n",
            "Epoch 10/20\n",
            "174/174 [==============================] - 4s 23ms/step - loss: 0.5007 - accuracy: 0.7595 - val_loss: 0.5105 - val_accuracy: 0.7492\n",
            "Epoch 11/20\n",
            "174/174 [==============================] - 4s 21ms/step - loss: 0.4983 - accuracy: 0.7529 - val_loss: 0.5080 - val_accuracy: 0.7424\n",
            "Epoch 12/20\n",
            "174/174 [==============================] - 3s 18ms/step - loss: 0.4967 - accuracy: 0.7529 - val_loss: 0.5343 - val_accuracy: 0.7273\n",
            "Epoch 13/20\n",
            "174/174 [==============================] - 3s 16ms/step - loss: 0.4954 - accuracy: 0.7640 - val_loss: 0.5086 - val_accuracy: 0.7462\n",
            "Epoch 14/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.4927 - accuracy: 0.7565 - val_loss: 0.5230 - val_accuracy: 0.7286\n",
            "Epoch 15/20\n",
            "174/174 [==============================] - 3s 19ms/step - loss: 0.4872 - accuracy: 0.7622 - val_loss: 0.5031 - val_accuracy: 0.7445\n",
            "Epoch 16/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.4857 - accuracy: 0.7619 - val_loss: 0.5101 - val_accuracy: 0.7336\n",
            "Epoch 17/20\n",
            "174/174 [==============================] - 3s 16ms/step - loss: 0.4841 - accuracy: 0.7669 - val_loss: 0.4968 - val_accuracy: 0.7496\n",
            "Epoch 18/20\n",
            "174/174 [==============================] - 3s 16ms/step - loss: 0.4800 - accuracy: 0.7714 - val_loss: 0.4999 - val_accuracy: 0.7475\n",
            "Epoch 19/20\n",
            "174/174 [==============================] - 4s 21ms/step - loss: 0.4757 - accuracy: 0.7689 - val_loss: 0.4936 - val_accuracy: 0.7475\n",
            "Epoch 20/20\n",
            "174/174 [==============================] - 3s 17ms/step - loss: 0.4733 - accuracy: 0.7748 - val_loss: 0.4914 - val_accuracy: 0.7546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfe38107700>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    }
  ]
}